{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Notebook - New Test\n",
    "This notebook will illustrate how to create and run a new experiment, or Test Run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "# Create a connection to the database\n",
    "db_connection = sqlite3.connect('experiment.db')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Creating a new Test Run\n",
    "Each experiment is represented by a Test Run, which will consume a Dataset to perform completions on a related set of Questions.\n",
    "\n",
    "The generated Responses and their Contexts will then be evaluated by the configured Test Eval functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new Test Run\n",
    "from packages.data.src.eval_data.models.testrun import TestRunModel, TestRunType\n",
    "\n",
    "test_run_model = TestRunModel(db_connection)\n",
    "test_run = test_run_model.add_or_get_test_run(TestRunType(datasource_id=3, description=\"Test Run 4\"))\n",
    "print(f\"Test Run added with ID: {test_run.id}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Load document text index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from packages.data.src.eval_data.models.document import DocumentModel\n",
    "from typing import List\n",
    "\n",
    "documents = DocumentModel(db_connection).get_documents_by_datasource(datasource_id=test_run.datasource_id)\n",
    "\n",
    "# Get questions and answers\n",
    "from packages.data.src.eval_data.models.question import QuestionModel, QuestionType\n",
    "from packages.data.src.eval_data.models.response import ResponseModel, ResponseType\n",
    "from packages.data.src.eval_data.models.context import ContextModel, ContextType\n",
    "\n",
    "questions: List[QuestionType] = []\n",
    "for doc in documents:\n",
    "    questions.extend(QuestionModel(db_connection).get_questions_by_document_id(document_id=doc.id))\n",
    "\n",
    "# Build the document index\n",
    "PERSIST_PATH = f\"datasets/query_engine/{test_run.datasource_id}\"\n",
    "\n",
    "from typing import List\n",
    "import os\n",
    "from documents import load_documents\n",
    "\n",
    "\n",
    "nodes = load_documents(documents)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create instance of the LLM query engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from packages.scripts.src.eval_scripts.utils import build_query_engine\n",
    "\n",
    "query_engine = build_query_engine(nodes, PERSIST_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Response Generation\n",
    "Utilizing the RAG system to generate responses to the questions during test runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate responses\n",
    "for q in questions:\n",
    "    res = query_engine.query(q.question)\n",
    "    response = ResponseType(\n",
    "        test_run_id = test_run.id,\n",
    "        question_id = q.id,\n",
    "        response = res.response\n",
    "    )\n",
    "    response_id = ResponseModel(db_connection).add_response(response)\n",
    "    print(f\"Response created: {response_id} - len: {len(response.response)}\")\n",
    "\n",
    "    context_model = ContextModel(db_connection)\n",
    "    for c in res.source_nodes:\n",
    "        context_model.add_context(\n",
    "            ContextType(\n",
    "                response_id=response_id, \n",
    "                text=c.node.get_content(),\n",
    "                similarity_score=c.score\n",
    "            )\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
