{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Notebook - New Test\n",
    "This notebook will illustrate how to create and run a new experiment, or Test Run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "# Create a connection to the database\n",
    "db_connection = sqlite3.connect('../experiment.db')\n",
    "\n",
    "# set info level logging\n",
    "from logging import basicConfig, INFO, getLogger\n",
    "basicConfig(level=INFO)\n",
    "logger = getLogger(__name__)\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Assuming the notebook is in the same directory as the 'custom' package\n",
    "sys.path.append(os.path.abspath('..'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Creating a new Test Run\n",
    "Each experiment is represented by a Test Run, which will consume a Dataset to perform completions on a related set of Questions.\n",
    "\n",
    "The generated Responses and their Contexts will then be evaluated by the configured Test Eval functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new Test Run\n",
    "from eval_data.models.testrun import TestRunModel, TestRunType\n",
    "from eval_data.models.datasource import DatasourceModel\n",
    "\n",
    "datasource = DatasourceModel(db_connection).get_datasource_by_name(\"WikiQA\")\n",
    "print(f\"Datasource ID: {datasource.id}\")\n",
    "\n",
    "test_run = TestRunModel(db_connection).add_or_get_test_run(\n",
    "    TestRunType(\n",
    "        datasource_id=datasource.id,\n",
    "        description=\"wiqiqa_norag_gpt35turbo\"\n",
    "    )\n",
    ")\n",
    "\n",
    "print(f\"Test Run ID: {test_run.id}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Load document Text and Questions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from eval_data.models.question import QuestionModel, QuestionType\n",
    "from eval_data.models.document import DocumentModel\n",
    "\n",
    "\n",
    "documents = DocumentModel(db_connection).get_documents_by_datasource(datasource_id=test_run.datasource_id)\n",
    "\n",
    "questions: List[QuestionType] = []\n",
    "for doc in documents:\n",
    "    questions.extend(QuestionModel(db_connection).get_questions_by_document_id(document_id=doc.id))\n",
    "\n",
    "print(f\"Loaded {len(documents)} documents\")\n",
    "print(f\"Loaded {len(questions)} questions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C: Save text and embeddings to db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eval_scripts.documents import load_documents, empty_set\n",
    "\n",
    "#nodes = load_documents(db_connection, documents)\n",
    "nodes = empty_set()\n",
    "\n",
    "print(f\"Loaded {len(nodes)} nodes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D: Create instance of the LLM query engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom.my_generator import build_query_engine\n",
    "\n",
    "import logging\n",
    "chroma_logger = logging.getLogger('chromadb')\n",
    "chroma_logger.setLevel(logging.ERROR)\n",
    "\n",
    "logger.info(f\"Building query engine with {len(nodes)} nodes\")\n",
    "query_engine = build_query_engine(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E: Response Generation\n",
    "Utilizing the RAG system to generate responses to the questions during test runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from packages.data.src.eval_data.models.response import ResponseModel, ResponseType\n",
    "from packages.data.src.eval_data.models.context import ContextModel, ContextType\n",
    "\n",
    "# Generate responses\n",
    "for q in questions:\n",
    "    res = query_engine.query(q.question)\n",
    "    response = ResponseType(\n",
    "        test_run_id = test_run.id,\n",
    "        question_id = q.id,\n",
    "        response = res.response\n",
    "    )\n",
    "    response_id = ResponseModel(db_connection).add_response(response)\n",
    "    print(f\"Response created: {response_id} - len: {len(response.response)}\")\n",
    "\n",
    "    context_model = ContextModel(db_connection)\n",
    "    for c in res.source_nodes:\n",
    "        context_model.add_context(\n",
    "            ContextType(\n",
    "                response_id=response_id, \n",
    "                text=c.node.get_content(),\n",
    "                similarity_score=c.score\n",
    "            )\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
